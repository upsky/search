---------------------
План:
--- До 20.06.2015:
[+] по запросу get=categories к веб-серверу отдавать список (иерархию) категорий.
[-] объекты, которые детектим на первом этапе (до 20.06):
    - цена;
    - срочность.
[+] попробовать символьные н-граммы;
    Качество немного падает; возможно это можно будет использовать, т.к. не нужна пословная морфология и
    исправление опечаток. Но контекстная морфология тут будет выигрывать наверняка.
[-] вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем какую-то категорию.

--- До 28.06.2015:
[-] объекты, которые детектим на первом этапе:
    - цена;
    + срочность.
[-] вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.

--- До 05.07.2015:
[+] Балансировка категорий. Лепим маленькие суб-категории (содержимым до ~10 примеров) к родительским.
[+] Возвращать структуру категорий + прокинуть это в сервер.
[+] Запилен фреймворк анализаторов поисковых запросов. Сделан первый маркер-тест.
[+] Обкачан юду:
    + слито по 100 страниц каждой категории верхнего уровня;
    + слит новый рукбрикатор, юду оказывается его меняет.
[-] Объекты, которые детектим на первом этапе:
    - Переделать urgency в обобощённый движок детекор объектов.
    - Цена;
[-] Прикрутить libshorttext или из sklearn, но вероятностный.
    Вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.

--- До 12.07.2015:
[+] Возвращаем несколько категорий (если есть);
[+] Возвращаем настоящую вероятность для категорий;
[+] Отсечение тупняков (пока по threshold'у).
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.
[-] Объекты, которые детектим на первом этапе:
    - Переделать urgency в обобощённый движок детекор объектов.
    - Цена;
[+/-] Прикрутить libshorttext - пока результаты не очень.
[-] По возможности возвращать топ терминов и их весов для списка категорий (?get=categories).

Конспект исследований:
1. Обучающие данные были получены обкачиванием, юду.
Затем sort | uniq
При таком раскладе качество вырастает хорошо.
2. Был посчитан margin для обучающей выбрки, выборка относительно не плохая, выбросов не очень много. Можно их выкинуть, посмотреть на качество (см learn/downloaders/youdo.com/scan_100pages_sorted_uniqed_margin.xlsx).
3. Балансировка обучающего множества в дереве категорий: balance_category_min=10 - при таком раскладе получается ~86 категорий и субкатегорий.
4. Как обучаем:
$> python learn_and_test.py exp --hier ml/learn/categories_youdo.com_2015.06.30.json --data ml/learn/downloaders/youdo.com/scan_100pages_sorted_uniqed.txt --key fit
В этой же утилите рассчёт марджина, тест классификатора.

--- До 21.07.2015:
[-] Новый ансамбль классификаторов:
    - векторайзер и классификатор для каждой категории - свой;
    - фиттинг векторайзера:
        - попробовать фитить только по обучающим примерам из своей категории;
        - посмотреть, что происходит с вектором, из каких весов он состоит, и что с весами для неизвестных слов? они выкидываются?
        - найти способ вытащить слова и их веса для конкретной категории.
    - построить марджины для обучающих примеров каждой категории, посмотреть, как меняется качество по к-фолдам, если убирать в разных пропорциях "выбросы" из обучающих примеров.
    - перенумеровать категории с 0, а не с 1;
    - сделать измерение качества каждой категории отдельно;
    - измерение качества по k-folds суммарно;
    - pipeline с подбором параметров?
[-] Таки допилить движок парсинга объектов.

---------------------
BackLog:
 + обкачать ещё youdo.com, т.к. не хватает обучающих примеров, если делать классификацию по суб-категориям.
 + ансамбли классифайеров:
    - ансамблинг разделением обучащего мн-ва;
    - ансамблинг по таргетам: кластеризация исходных категорий по обуч мн-ву, выделение синтетических пар над-категорий:
        1. пара: в каждой из категорий находятся наиболее похожие исх.кат;
        2. пара: в каждой из категорий равн.распред-ные противоположные исх.категории в пропорции 50-50;
        3. пара: ... другие смешанные синтетики в прочих пропорциях: 40-60, 30-70, 20-80... с каким-то заранее заданным шагом.
      далее отсечение.
 - в сервер добавить запрос "ver=what", по которому он выдаёт версию с описанием возможностей.
 - заюзать нормальную морфологию + с учётом соседних слов, https://github.com/kmike/pymorphy2
 - попробовать заюзать что-нибудь другое, кроме bag-of-words;
 - try TF-IDF for categories instead of dox;
 + отделить обучалку/тестилку от предиктора, причесать сервер;
 - grid-search, составить таблицу: F1 - algo - algo params;
 + play around the 'analyzer' and 'token normalization' under CountVectorizer;
 - TruncatedSVD, latent semantic analysis???

---------------------
Параметры поиска:
 - услуга (специализация, категория)
 - цена
 - место: город, метро, улица, район
 - время (срочно, утром, днём, вечером, сегодня, завтра, на этой неделе, в этом месяце, в следующем...);
 - способ предоставления услуги: у спеца, у меня (на дому, дома, домой), он-лайн, по скайпу, по телефону, удалённо
 - флаг: записаться онлайн
 - флаг: только с [положительными|хорошими] отзывами
 - пол спеца: мужчина/женщина
 - флаг: недорого, дёшево, дорого, люкс
 - флаг: под ключ;

Признаки:
 - что сделать: починить колесо, вылечить насморк;
 - симптом: сдулось колесо, болит голова, головная боль, насморк;
 - специализация: хорошего врача, врача на дом, сантехник;

Структура:
 - нормализатор (е-ё, lower-case)
 - опечаточник - отдельная задача;
 - морфологический нормализатор - для начала просто стеммер;
 - классификатор (скорее всего несколько классификаторов, каждый вытаскивает свой признак);

---------------------


---------------------
Акинатор - http://ru.akinator.com/:
    - можно построить прикольную игровую штуку, которая будет "угадывать" то, что хочет пользователь.
    - таким-то образом можно на самом деле вытаскивать из реальных пользователей онтологию того, как и из чего состоят их потребности.
    - продумать бы каким образом задавать пользователю так, чтобы убеждаться не только в правильности готовых вопросов, но и простимулировать их к формированию новых, если их в системе пока нет.

---------------------
Thinks:
2015.07.20
    Experiments:
    1. Загружаем данные одной категории
    2. Фиттим на них векторайзер
        - посмотреть, что там внутри векторайзера
        - странсофрмить на векторайзере незнакомый текст - что там внутри?
            = На месте всех известных фич - нули.
    3. Фиттим на них классифайер
    4. Делаем к-фолд на этом классифайере

    Done, выводы:
    = К-фолд показывает достаточно высокую точность на каждой категории в отдельности.
      Но тогда вопрос: не переобучаемся ли мы?
    = Если пробуем К-фолд на ансамбле (OneVsTheRest)
    = Тот факт, что у нас юзаются слова, трудно говорить о переобучении - слова либо совпадают, либо нет.
      Однако какие-то слова имеют больший вес и они решают, а другие меньший вес, и к ним больше терпимости.
      Отсюда два вывода:
       - хорошо бы иметь синонимайзер. Для этого можно заюзать word2vec, чтобы попробовать
         поопределять т.н. квази-синонимы.
         Как вставлять синонимы в запрос:
          - для классификтора можно по идее просто отгружать все такие фичи в одну кучу. Возможно это будет шумом.
          - либо на каждый суб-инстанс запроса, сгенерённый по синонимам, отправлять всё это дело в классификатор,
            а дальше выбирать результат голосованием.
       - в качестве ядра заюзать вообще Баеса и сравнить с СГД-ядром; предполагаю, что байес будет
         не то что не хуже, а лучше на реальных данных.

    ----
    1. Построить свой классифайер с sklearn-подобным интерфейсом, методы:
        fit(X, y)
        predict(X)
        predict_proba(X)
        а). на каждый класс делается свой классификатор с заданным ядром
            (инстанс классификатора передаётся параметром? как копировать инстансы? посмотреть в sklearn)
        б). sklearn.multiclass.OneVsRestClassifier - уже делает это.
    2. Сравнить с обычным СГДклассифайером:
       Замерить к-фолд на ВСЕХ обучающих данных на новом ансамбль-классифайере.
    3. я не заметил особой разницы между тем, когда фиттиш векторайзер по всем текстам или только по текстам категории.
       Думаю, разница будет, когда будем обучаться на доках из других категорий.
    4. Я заметил большие странности с Tfidf-векторайзером
        - он вычисляет очень странные меры, которые неочевидным образом зависят от текста слова (!);
        - он как-то очень слабо использует stop_words, как будто и не выкидывает их совсем. Пример: берём запрос
          "срочно доставить документы" и сравниваем его с "доставить документы". Первый запрос получается с шумом из-за
          слова "срочно". Добавляем слово "срочно" в стоп-слова, пробуем - вероятность от классификатора повыше, но
          НЕ такая же, как и просто без него, не дотягивает.
          Это ж удивительно.
    5. затем заметил, что конечно есть разница между тем, когда фильтруешь содержимое категории по маржину.
       Но тут опять вопрос: не переобучаемс ли.
       И похоже переобучаемся, т.к. мы фитимся и затем детектим запросы совсем из других категорий.
    '''

-- 2015.08.11
Распознавание речи:
1) Яндекс
  FAQ:
  https://tech.yandex.ru/speechkit/mobilesdk/doc/intro/faq/concepts/About-docpage/
  Вот здесь описание возможностей Mobile SDK:
    https://tech.yandex.ru/speechkit/mobilesdk/
    в бизнес-версии есть:
      - оффлайн-распознавание;
        зачем: чтобы можно было распознавать голос, преобразовывать в текст, а далее отправлять хоть по смс.
        вопрос: сколько либа со всей базой распознавания потребует места на самом мобильнике?
      - определение тематики;
      - выделение типовых объектов: имена и фамилии, адреса, даты, время.
      - заявлено, что могут подстроить всё под бизнес.
  Установил Яндекс.Диктовку под Андроид. Качество распознавания речи вполне сносное.

2) Гугл
  У них есть API, встроенный в гугл.хром:
  Демка Web-Speech-Api: https://www.google.com/intl/en/chrome/demos/speech.html
  Спецификация на W3C: https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html
  Выдержка из спецификации: "This specification defines a JavaScript API to enable web developers to incorporate speech recognition and synthesis into their web pages."
  Community: https://www.w3.org/community/speech-api/
  А вообще у них нет какого-то документированного явно открытого API, чтобы можно было юзать отдельно от браузера.
  Видно, что они закрыли его, потому что раньше он вроде как был, потому что он присутствует в списке вот здесь: http://www.chromium.org/developers/how-tos/api-keys, но уже отсутствует в списке, когда регишься (https://console.developers.google.com/project).
  Однако браузер open-source, поэтому умельцы быстренько раскопали, как к нему обратиться:
  http://habrahabr.ru/post/144535/
  С другой стороны, вот есть относительно официальный питон-пакет, которые юзает этот АПИ:
  https://pypi.python.org/pypi/SpeechRecognition/
  Попробовал, вполне себе работает. Строго рекомендуется завести свой API-ключ (см. доку на странице).
  Итого: всё это выглядит не очень надёжно, т.к. не имеет официальной поддержки и API за деньги.

3) Есть другие наши отечественные разработчики. Например:
  http://www.speechpro.ru/technologies/recognition
При чём, как видно ребята вообще с безопасниками и стенографистами обращаются, поэтому их качество скорее всего действительно должно быть на высоте. Да правда при условии "гос-взаимодействия", пахнет нереально высокими ценами. Пока не общался и не копал.

4) Наконец, можно самим запилить:
  http://habrahabr.ru/post/237589/
Для полноценного распознавания конечно придётся пройти полный путь яндекса или гугла (500 часов диктовок и текста, всё это обрабатывать и т.п.), что выглядит совершенно нереально и ненужно. Однако можно заюзать для распознавания какого-то фиксированного набора команд, типа стартового обращения "ОКей гугл".

== Итого: распознавание речи скорее всего нужно будет делать на связке: Yandex SpeechKit + возможно обучить свою маленькую для коротких команд.


Обработка текстов, выделение объектов.
Разработчики и исследователи:
0) Эксперты в области лингвистики:
  - https://nlpub.ru/Персоналии - частные эксперты
  - https://nlpub.ru/Организации - российские компании
  - Другие ребята, которых я находил:
    - http://habrahabr.ru/post/229403/ (у него ещё есть другие тоже интересные статьи);

1) MeanoTek - ребята запилили свой краулер и поисковик по товарам (http://reviewdot.ru/), соответственно выделяют объекты в тексте, тональность текста в отзывах, определяют качество товаров, выделяя для этого характеристики и оценки по этим харак-кам:
  http://habrahabr.ru/company/meanotek/
  http://meanotek.ru/
  Довольно интересно пишут, рассуждают и немалое заявляют.
  Взять например их документ: http://meanotek.ru/files/text-analysis.pdf - на второй страничке внизу табличка из двух строк - весьма интересный результат предлагают. Если они действительно способны такое вычленять, то с ними стоит поработать. К тому же, хоть у них на сайте телефон и московский, вот в этом докладе одного из чуваков я видел, что он из Казани: http://www.meanotek.ru/files/TarasovDS(2)2015-Dialogue.pdf ))
  https://vk.com/id294582392 - 1ый из Казани
  https://vk.com/solanov - 2ой из Москвы
  Зарегился: Ваш API ключ: 55caf1b79126b
  Вообще общее впечатление о ребятах смешанное: я у них то и дело нахожу опечатки, баги (уже зарепортил штуки 3 наверное), на что они ссылаются на заказных работников, которые делали им сайты. Надо потестить сам движок.

2). TextoCat, пока сыроваты, не переписывался, нужно разговаривать:
  1. http://habrahabr.ru/company/textocat/profile/
     Заявлено два продукта:
      - TextoCat - предоставляеют его через API: https://github.com/textocat, сейчас пока вроде бесплатно.
      - TextoKit - заявлена (но пока не готова), либа с открытым исходным кодом: http://textocat.ru/textokit.html
  2. Доступна демка: http://textocat.ru/demo/ - пробовал на коротких текстах, сейчас печальненько, где-то на сайте видел, что они сейчас заточены пока что под новости. На чём проверял:
    - Выделил:
      нужен репетитор по английскому, не дороже 1000 рублей в час (1000 рублей пометил как деньги)
      Срочно мастера - сантехника на улицу Герцена 19 (выделил "улица Герцена" как "здание", а вместе с "19" как "место")
    - Странновато:
      Няня со стажем работы от 5 лет на один вечер срочно ("няня"="чел", "от 5 лет на один вечер"=всё вместе как "время")
      На вечер пятницы нужна сиделка. Ребенок 5 лет на 4-5 часов. ("на вечер пятницы"="время"-ОК, "5 лет на 4-5 часов" - выделил как время)
    - False-Positive:
      срочно! газовик на адрес Герцена 19 (выделил "герцена" как "человек")
      Нужен учитель китайского языка по будням на дом с нуля (выделил "дом", как здание и всё)
      Я на пересечение маркса и масленикова. Белый фольг 222, у меня нет насоса. (выделил "я" и "меня" как "чел"??)
    - Ничего не выделил:
      срочно! забился унитаз, дом топит, адрес ул. Большая Никитская, д. 10
      забрать пачку документов с м. Лермонтово и привезти по адресу ул. Сухаревская, д. 8
  3. Есть форум: http://feedback.textocat.com/

3). DaData, https://dadata.ru/
  1. Заявляют, что хорошо исправляют (но похоже не могут ли извлекать из текста) следующие объекты:
    - ГЕО-адреса
    - Имена
    - Телефоны
    И всё это могут исправлять в связке: например, по адресу проставляют код города в номере телефона.
    А посему полезные для всяких инет-магазов, которые работают по наложенному платежу (заполнение почтовых отправлений), ну или просто там, где нужно заполнять и проверять точность адреса.
  2. Есть демка так называемых подсказок, пока не пробовал, 10К запросов в день бесплатно: https://dadata.ru/suggestions/#pricing


Разработки, которые мы можем использовать:
1) Томита-парсер:
  OpenSource от Yandex, Очень хорошая штука, которую можно настроить самостоятельно.
  Много возможностей для описания правил извелчения объектов (фактов) с помощью контекстно-свободных грамматик. Обладает встроенной поддержкой морфологии, и прочих лингвиситических приблуд. Как минимум его можно использовать при извелечении объектов.
  Вот здесь хороший краткий туториал: https://tech.yandex.ru/tomita/doc/video/index-docpage/
  Вот здесь посмотреть, как парень его юзает и что юзает ещё: http://habrahabr.ru/post/229403/


Взаимодействие с пользователями (отдельная большая задача):
1) Сравнение Siri (iPhone), OK Google (Android), Cortana (WinPhone):
  http://www.youtube.com/watch?v=k-XaP2Msudg
  Мой вывод из сравнения, что больше всего удовлетворяет пользователя:
  - точный ответ без дополнительных вопросов;
  - нужен ответ на конкретно поставленный вопрос, а не какая-нибудь отсылка к поисковой выдаче.

2) "Дуся":
  http://www.youtube.com/watch?v=B3ygkiXEurY
  http://4pda.ru/forum/index.php?showtopic=561485
  Автор интересен, для распознавания речи очевидно использует гугловый SpeechAPI.
  Вопросы к автору:
    - использует ли он для диалогов машинное обучение, либо у него стандартный набор зашитых диалогов. Т.е. на сколько он близок к теме построения настоящих "мозгов", а не простого конечного автомата.
    - дуся доступня для скачивания, нужно протестить, пока не смотрел.


ТОДО:
  - узнать, сколько будет занимать место на мобиле Yandex SpeechKit Mobile, если его установить локально. Есть подозрение, что Яндекс.Диктовка - это уже всё, что надо, т.к. вроде как Android JellyBean уже тоже умеет без инета распознавать. Try: отключить инет на планшете и подиктовать; если работает - посмотреть, сколько весит эта прилага.
  - посмотреть Дусю, побеседовать с её разработчиком (см. вопросы выше).
  - потестить API meanotek на предмет автоматического обучения извлечения объектов, как у них описано в статье: http://habrahabr.ru/company/meanotek/blog/258211/ ; если будет время - сравнить с CRFSuite: http://www.chokkan.org/software/crfsuite/tutorial.html - он использует не нейронки, а CRF (Conditional Random Fields).
  - потестить и поизучать Томита-парсер, попробовать запилить на нём извлечение цены и срочности.

