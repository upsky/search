---------------------
План:
--- До 20.06.2015:
[+] по запросу get=categories к веб-серверу отдавать список (иерархию) категорий.
[-] объекты, которые детектим на первом этапе (до 20.06):
    - цена;
    - срочность.
[+] попробовать символьные н-граммы;
    Качество немного падает; возможно это можно будет использовать, т.к. не нужна пословная морфология и
    исправление опечаток. Но контекстная морфология тут будет выигрывать наверняка.
[-] вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем какую-то категорию.

--- До 28.06.2015:
[-] объекты, которые детектим на первом этапе:
    - цена;
    + срочность.
[-] вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.

--- До 05.07.2015:
[+] Балансировка категорий. Лепим маленькие суб-категории (содержимым до ~10 примеров) к родительским.
[+] Возвращать структуру категорий + прокинуть это в сервер.
[+] Запилен фреймворк анализаторов поисковых запросов. Сделан первый маркер-тест.
[+] Обкачан юду:
    + слито по 100 страниц каждой категории верхнего уровня;
    + слит новый рукбрикатор, юду оказывается его меняет.
[-] Объекты, которые детектим на первом этапе:
    - Переделать urgency в обобощённый движок детекор объектов.
    - Цена;
[-] Прикрутить libshorttext или из sklearn, но вероятностный.
    Вероятностная классификация - вектор категорий, или хотя бы вероятность по той, которую задетектили.
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.

--- До 12.07.2015:
[+] Возвращаем несколько категорий (если есть);
[+] Возвращаем настоящую вероятность для категорий;
[+] Отсечение тупняков (пока по threshold'у).
    Цель: не хорошо, когда мы по какой-нибудь фигне показываем конкретную категорию.
[-] Объекты, которые детектим на первом этапе:
    - Переделать urgency в обобощённый движок детекор объектов.
    - Цена;
[+/-] Прикрутить libshorttext - пока результаты не очень.
[-] По возможности возвращать топ терминов и их весов для списка категорий (?get=categories).

Конспект исследований:
1. Обучающие данные были получены обкачиванием, юду.
Затем sort | uniq
При таком раскладе качество вырастает хорошо.
2. Был посчитан margin для обучающей выбрки, выборка относительно не плохая, выбросов не очень много. Можно их выкинуть, посмотреть на качество (см learn/downloaders/youdo.com/scan_100pages_sorted_uniqed_margin.xlsx).
3. Балансировка обучающего множества в дереве категорий: balance_category_min=10 - при таком раскладе получается ~86 категорий и субкатегорий.
4. Как обучаем:
$> python learn_and_test.py exp --hier ml\learn\categories_youdo.com_2015.06.30.json --data ml\learn\downloaders\youdo.com\scan_100pages_sorted_uniqed.txt --key fit
В этой же утилите рассчёт марджина, тест классификатора.


---------------------
BackLog:
 + обкачать ещё youdo.com, т.к. не хватает обучающих примеров, если делать классификацию по суб-категориям.
 + ансамбли классифайеров:
    - ансамблинг разделением обучащего мн-ва;
    - ансамблинг по таргетам: кластеризация исходных категорий по обуч мн-ву, выделение синтетических пар над-категорий:
        1. пара: в каждой из категорий находятся наиболее похожие исх.кат;
        2. пара: в каждой из категорий равн.распред-ные противоположные исх.категории в пропорции 50-50;
        3. пара: ... другие смешанные синтетики в прочих пропорциях: 40-60, 30-70, 20-80... с каким-то заранее заданным шагом.
      далее отсечение.
 - в сервер добавить запрос "ver=what", по которому он выдаёт версию с описанием возможностей.
 - заюзать нормальную морфологию + с учётом соседних слов, https://github.com/kmike/pymorphy2
 - попробовать заюзать что-нибудь другое, кроме bag-of-words;
 - try TF-IDF for categories instead of dox;
 + отделить обучалку/тестилку от предиктора, причесать сервер;
 - grid-search, составить таблицу: F1 - algo - algo params;
 + play around the 'analyzer' and 'token normalization' under CountVectorizer;
 - TruncatedSVD, latent semantic analysis???

---------------------
Параметры поиска:
 - услуга (специализация, категория)
 - цена
 - место: город, метро, улица, район
 - время (срочно, утром, днём, вечером, сегодня, завтра, на этой неделе, в этом месяце, в следующем...);
 - способ предоставления услуги: у спеца, у меня (на дому, дома, домой), он-лайн, по скайпу, по телефону, удалённо
 - флаг: записаться онлайн
 - флаг: только с [положительными|хорошими] отзывами
 - пол спеца: мужчина/женщина
 - флаг: недорого, дёшево, дорого, люкс
 - флаг: под ключ;

Признаки:
 - что сделать: починить колесо, вылечить насморк;
 - симптом: сдулось колесо, болит голова, головная боль, насморк;
 - специализация: хорошего врача, врача на дом, сантехник;

Структура:
 - нормализатор (е-ё, lower-case)
 - опечаточник - отдельная задача;
 - морфологический нормализатор - для начала просто стеммер;
 - классификатор (скорее всего несколько классификаторов, каждый вытаскивает свой признак);

---------------------


---------------------
Акинатор - http://ru.akinator.com/:
    - можно построить прикольную игровую штуку, которая будет "угадывать" то, что хочет пользователь.
    - таким-то образом можно на самом деле вытаскивать из реальных пользователей онтологию того, как и из чего состоят их потребности.
    - продумать бы каким образом задавать пользователю так, чтобы убеждаться не только в правильности готовых вопросов, но и простимулировать их к формированию новых, если их в системе пока нет.

